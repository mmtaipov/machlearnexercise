import pandas as pd
import catboost

import numpy as np
from sklearn.model_selection import KFold
from copy import deepcopy
from tqdm.notebook import tqdm
import os
from sklearn.preprocessing import StandardScaler
from catboost import CatBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV

train = pd.read_csv("/Users/mikhailtaipov/Downloads/ef-msu-master-comp3/train.csv")
test = pd.read_csv("/Users/mikhailtaipov/Downloads/ef-msu-master-comp3/test.csv")
sample=pd.read_csv("/Users/mikhailtaipov/Downloads/ef-msu-master-comp3/sample_submission.csv")
test_ids = test["id"]
def mape(y,p):
    return np.nanmean(np.divide(np.abs((y - p)), np.abs(y)))

train["checkout_price"]=train["checkout_price"]-train["checkout_price"].mean()
train["checkout_price"]=train["checkout_price"]/(train["checkout_price"].std())
train["base_price"]=train["base_price"]-train["base_price"].mean()
train["base_price"]=train["base_price"]/(train["base_price"].std())
 
train["rating"]=train["rating"]-train["rating"].mean()
train["rating"]=train["rating"]/(train["rating"].std())
train["op_area"]= (np.round(train["op_area"])).astype(int)
train["relprice"]=train["checkout_price"]/train["base_price"]
 
train.head()

id	week	center_id	meal_id	checkout_price	base_price	emailer_for_promotion	homepage_featured	rating	city_code	region_code	center_type	op_area	category	cuisine	num_orders	relprice
0	1379560	1	55	1885	-1.270654	-1.257652	0	0	-0.418834	647	56	TYPE_C	2	Beverages	Thai	177	1.010338
1	1466964	1	55	1993	-1.270654	-1.361643	0	0	-0.114370	647	56	TYPE_C	2	Beverages	Thai	270	0.933177
2	1346989	1	55	2539	-1.283632	-1.361453	0	0	0.066914	647	56	TYPE_C	2	Beverages	Thai	189	0.942840
3	1338232	1	55	2139	0.064550	0.544444	0	0	-0.516448	647	56	TYPE_C	2	Beverages	Indian	54	0.118561
4	1448490	1	55	2631	-0.567905	-0.687721	0	0	-0.700056	647	56	TYPE_C	2	Beverages	Indian	40	0.825777

test["checkout_price"]=test["checkout_price"]-test["checkout_price"].mean()
test["checkout_price"]=test["checkout_price"]/(test["checkout_price"].std())
test["base_price"]=test["base_price"]-test["base_price"].mean()
test["base_price"]=test["base_price"]/(test["base_price"].std())
 

test["rating"]=test["rating"]-test["rating"].mean()
test["rating"]=test["rating"]/(test["rating"].std())
test["relprice"]=test["checkout_price"]/test["base_price"]
 
test["op_area"]= (np.round(test["op_area"])).astype(int)
test.head()
	id	week	center_id	meal_id	checkout_price	base_price	emailer_for_promotion	homepage_featured	rating	city_code	region_code	center_type	op_area	category	cuisine	relprice
0	1256496	100	55	1885	-1.243297	-1.289691	0	0	0.425949	647	56	TYPE_C	2	Beverages	Thai	0.964027
1	1298009	100	55	1993	-1.211645	-1.272098	0	0	-1.148432	647	56	TYPE_C	2	Beverages	Thai	0.952478
2	1348875	100	55	2539	-1.205779	-1.254505	0	0	1.010758	647	56	TYPE_C	2	Beverages	Thai	0.961159
3	1272771	100	55	2139	-0.192781	-0.316568	0	0	-0.433665	647	56	TYPE_C	2	Beverages	Indian	0.608970
4	1313265	100	55	2631	-1.549311	-1.242957	0	0	0.195816	647	56	TYPE_C	2	Beverages	Indian	1.246472

def cv_and_predict(
    df_train,
    df_test,
    train_y,
    model,
    log_transform_target=True,
    do_scaling=False,
    n_splits=5,
    random_state=42,
    verbose=True,
):
    """
    Функция для кросс-валидации и предикта на тест

    :param df_train: Трейн-датафрейм
    :param df_test: Тест-датафрейм
    :param train_y: Ответы на трейн
    :param model: Модель, которую мы хотим учить
    :param log_transform_target: Делаем ли лог-трансформацию таргета при обучении
    :param do_scaling: Делаем ли скейлинг признаков
    :param n_splits: Количество сплитов для KFold
    :param random_state: random_state для KFold
    :param verbose: Делаем ли print'ы

    :return: pred_test: Предсказания на тест; oof_df: OOF предсказания
    """

    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    # В датафрейме oof_df будут храниться настоящий таргет трейна и OOF предсказания на трейн.
    # Инициализируем prediction_oof нулями и будем заполнять предсказаниями в процессе валидации
    oof_df = pd.DataFrame()
    oof_df["target"] = train_y
    oof_df["prediction_oof"] = np.zeros(oof_df.shape[0])

    # Список с метриками по фолдам
    metrics = []

    # Предсказания на тест. Инициализируем нулями и будем заполнять предсказаниями в процессе валидации.
    # Наши предсказания будут усреднением n_splits моделей
    pred_test = np.zeros(df_test.shape[0])

    # Кросс-валидация
    for i, (train_index, valid_index) in enumerate(kf.split(df_train, train_y)):
        if verbose:
            print(f"fold_{i} started")

        X_train = df_train.loc[train_index]
        y_train = train_y.loc[train_index].values

        if do_scaling:
            scaler = StandardScaler()
            columns = X_train.columns
            X_train = scaler.fit_transform(X_train)
            X_train = pd.DataFrame(X_train)
            X_train.columns = columns

        X_valid = df_train.loc[valid_index]
        y_valid = train_y.loc[valid_index].values

        if log_transform_target:
            y_train = np.log1p(y_train)
            y_valid = np.log1p(y_valid)

        if do_scaling:
            X_valid = scaler.transform(X_valid)
            X_valid = pd.DataFrame(X_valid)
            X_valid.columns = columns

        model_kf = deepcopy(model)

        model_kf.fit(
            X_train,
            y_train,
            plot=False,
            eval_set=(X_valid, y_valid),
            use_best_model=True,
            early_stopping_rounds=50,
        )

        if do_scaling:
            df_test_scaled = scaler.transform(df_test)
            df_test_scaled = pd.DataFrame(df_test_scaled)
            df_test_scaled.columns = columns
            prediction_kf = model_kf.predict(df_test_scaled)
        else:
            prediction_kf = model_kf.predict(df_test)

        if log_transform_target:
            prediction_kf = np.expm1(prediction_kf)

        pred_test += prediction_kf / n_splits

        prediction = model_kf.predict(X_valid)

        if log_transform_target:
            prediction = np.expm1(prediction)
            y_valid = np.expm1(y_valid)
        oof_df.loc[valid_index, "prediction_oof"] = prediction

        cur_metric = mape(y_valid, prediction)
        metrics.append(cur_metric)
        if verbose:
            print(f"metric_{i}: {cur_metric}")
            print()
            print("_" * 100)
            print()

    metric_OOF = mape(train_y, oof_df["prediction_oof"])

    if verbose:
        print(f"metric_OOF: {metric_OOF}")
        print(f"metric_AVG: {np.mean(metrics)}")
        print(f"metric_std: {np.std(metrics)}")
        print()
        print("*" * 100)
        print()

    return pred_test, oof_df, metric_OOF

cbs_reg = CatBoostRegressor(
    loss_function="LogLinQuantile",
    learning_rate=0.9,
    
    random_state=1337,
    thread_count=-1,
    num_trees=300,
    cat_features=["city_code", "region_code", "center_type", "category", "cuisine","homepage_featured","emailer_for_promotion"],
    verbose=200,depth=12,langevin=True,      
)
pred_test, oof_df, metric_OOF = cv_and_predict(train.drop(['num_orders', 'id', 'week'], axis=1),
                                               test.drop(['id', 'week'], axis=1), 
                                               train_y=train['num_orders'], 
    n_splits=5, model=cbs_reg)
fold_0 started
0:	learn: 1.6630458	test: 1.6614723	best: 1.6614723 (0)	total: 265ms	remaining: 1m 19s
Stopped by overfitting detector  (50 iterations wait)

bestTest = 0.899450403
bestIteration = 1

Shrink model to first 2 iterations.
metric_0: 0.9665658827782178

____________________________________________________________________________________________________

fold_1 started
0:	learn: 1.6630104	test: 1.6617854	best: 1.6617854 (0)	total: 233ms	remaining: 1m 9s
Stopped by overfitting detector  (50 iterations wait)

bestTest = 0.8998971585
bestIteration = 1

Shrink model to first 2 iterations.
metric_1: 0.9665259547157415

____________________________________________________________________________________________________

fold_2 started
0:	learn: 1.6625500	test: 1.6635271	best: 1.6635271 (0)	total: 233ms	remaining: 1m 9s
Stopped by overfitting detector  (50 iterations wait)

bestTest = 0.9008157746
bestIteration = 1

Shrink model to first 2 iterations.
metric_2: 0.9667008738557206

____________________________________________________________________________________________________

fold_3 started
0:	learn: 1.6618571	test: 1.6658820	best: 1.6658820 (0)	total: 245ms	remaining: 1m 13s
Stopped by overfitting detector  (50 iterations wait)

bestTest = 0.9024810983
bestIteration = 1

Shrink model to first 2 iterations.
metric_3: 0.9667935873570747

____________________________________________________________________________________________________

fold_4 started
0:	learn: 1.6630843	test: 1.6609177	best: 1.6609177 (0)	total: 230ms	remaining: 1m 8s
Stopped by overfitting detector  (50 iterations wait)

bestTest = 0.8987022403
bestIteration = 1

Shrink model to first 2 iterations.
metric_4: 0.9664144932687869

____________________________________________________________________________________________________

metric_OOF: 0.9666001583697353
metric_AVG: 0.9666001583951083
metric_std: 0.00013319978259024933

****************************************************************************************************



submission = pd.DataFrame()
submission["id"] = test_ids
submission["num_orders"] = pred_test
submission
	id	num_orders
0	1256496	2.131574
1	1298009	2.145528
2	1348875	2.131574
3	1272771	2.167109
4	1313265	2.127053
5	1392835	2.163363
6	1405813	2.129436
7	1294858	2.140194
8	1115085	2.148728
9	1423887	2.147299
10	1250994	2.147299
11	1204955	2.163165
12	1129621	2.162736
13	1197225	2.141298
14	1377833	2.152009
15	1378056	2.164358
16	1055278	2.117456
17	1447206	1.573747
18	1099276	2.131654
19	1275326	2.158856
20	1006926	2.161854
21	1040731	2.138819
22	1023036	2.129588
23	1263417	2.142493
24	1147662	2.143069
25	1164854	2.168533
26	1287418	2.155238
27	1403855	2.164935
28	1441310	1.846864
29	1198477	2.157799
...	...	...
150530	1325626	2.171285
150531	1093269	2.170877
150532	1338824	2.167210
150533	1158460	2.165535
150534	1340813	2.167340
150535	1443704	2.171285
150536	1287722	2.171443
150537	1031000	2.171103
150538	1288462	2.170656
150539	1220280	2.166571
150540	1032849	2.166353
150541	1469586	2.166115
150542	1038490	2.165736
150543	1394189	2.166464
150544	1357555	2.165128
150545	1176475	2.170815
150546	1133784	2.167210
150547	1379712	2.167340
150548	1306192	2.169534
150549	1371867	2.167917
150550	1055597	2.168762
150551	1372838	2.165924
150552	1035758	2.169728
150553	1010438	2.171443
150554	1116711	2.171155
150555	1271326	2.170578
150556	1062036	2.171155
150557	1110849	2.170198
150558	1147725	2.170049
150559	1361984	2.166702
150560 rows × 2 columns
submission.to_csv("/Users/mikhailtaipov/Downloads/submission.csv", index=False)
